\section{Introduction}
\label{sec:introduction}

Recurrent neural networks \cite[RNNs,][]{Elman:1990}, in particular
their Long-Short-Term-Memory variant
\cite[LSTMs,][]{Hochreiter:Schmidhuber:1997}, are the current
workhorse of natural language processing. These models, often
pre-trained on the simple \emph{language modeling} objective of
predicting the next symbol in raw natural text, form a crucial
component of state-of-the-art architectures for tasks such as machine
translation, natural language inference and text categorization
\cite{Goldberg:2017}.

RNNs are very general devices for sequence processing, assuming little
prior bias. Moreover, the simple prediction task they are trained on
in language modeling seems well-aligned with the core role prediction
plays in cognition \cite[e.g.,][]{Bar:2007,Clark:2016}. RNNS have thus
long attracted the attention of cognitive scientists and linguists
interested in language and processing, and their recent successes in
realistic large-scale tasks has strongly rekindled this interest
\cite[see, e.g.,][and references there]{Frank:etal:2013,Lau:etal:2017,Kirov:Cotterell:2018,McCoy:etal:2018,Pater:2018}.

Following the standard pre-processing pipeline for RNNs, these studies
assume that the input has been tokenized into words, and the latter
are pre-stored in the RNN vocabulary. This is a reasonable practical
approach, but it makes simulations less interesting from a language
learning point of view. First, discovering words is one of the major
challenges a learner faces, and by pre-encoding them in the RNN we are
facilitating its task in a very unnatural way (not even the staunchest
nativists would claim words to be part of our genetic code). Second,
assuming a unique tokenization into a finite number of discrete word
units is in any case problematic. The very notion of what counts as a
word in languages with a rich morphology is far from clear
\cite[e.g.,][]{Bickel:Zuniga:2017}, and, universally, mental lexicons
are probably organized into a not-necessarily-consistent hierarchy of
units at different levels: morphemes, words, compounds, constructions,
etc.~\cite[e.g.,][]{Goldberg:2005}.

Motivated by these considerations, we present here an extensive study
of RNNs trained on language modeling at the character level, or
\emph{character-level neural language models}
\cite[CNLMs,][]{Mikolov:etal:2011,Sutskever:etal:2011,Graves:2014}. Moreover,
the RNNs are trained on input where whitespace has been removed, so
that, like children learning a language, they don't have access to
major cues to wordhood.\footnote{We do not erase punctuation marks,
  reasoning that they have a similar function to prosodic cues in
  spoken language.} This setup is almost as \emph{tabula rasa} as it
goes: by taking unsegmented orthographic output (and assuming that, in
the alphabetic writing systems we work with, there is a reasonable
correspondence between letters and phonetic segments), we are only
assuming that a learner has figured out how to segment a continuous
speech stream into phonological units, an ability that children a few
months after birth \cite[e.g.,][]{Maye:etal:2002,Kuhl:2004}.

Our simulations involve phonological, morphological, syntactic and
semantic tests in English, German and Italian. Taken together, they
show that near-\emph{tabula rasa} CNLMs acquire an impressive spectrum
of linguistic knowledge at various levels.  This in turn suggests
that, given abundant input (large Wikipedia dumps), a learning device
whose only prior architectural bias consists in the LSTM cell to
propagate memory across time steps, can implicitly acquire a variety
of linguistic rules that one would intuitively expect to require much
more prior knowledge. One aspect that we find particularly intriguing
is that, unlike word-based models, our CNLMs do not have a morpheme-
or word-based lexicon. Any information the network might posses about
units larger than characters must be stored in its recurrent
weights. Given that nearly all contemporary linguistics recognizes a
central role to the lexicon \cite[see, e.g.,][for different
perspectives]{Sag:etal:2003,Radford:2006,Bresnan:etal:2016,Jezek:2016},
in future work we would like to explore the new view of how lexical
knowledge might be implicitly encoded in memory that is suggested by
the encouraging results obtained by our CNLMs.



