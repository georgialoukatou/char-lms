\section{Introduction}
\label{sec:introduction}

Recurrent neural networks (RNNS), in particular their Long-Short-Term-Memory
variant \cite[LSTMs,][]{Hochreiter:Schmidhuber:1997} are the current workhorse
of natural language processing. These models, often pre-trained on the
simple \emph{language modeling} objective of predicting the next
symbol in raw natural text, form a crucial component of
state-of-the-art architectures for tasks such as machine translation,
natural language inference and text categorization
\cite{Goldberg:2017}.

RNNs are very general devices for sequence processing, assuming little
prior bias. Moreover, the simple prediction task they are trained on
in language modeling seems well-aligned with the core role prediction
plays in cognition \cite[e.g.,][]{Bar:2007,Clark:2016}. RNNS have thus
long attracted the attention of cognitive scientists and linguists
interested in language and processing, and their recent successes in
realistic large-scale tasks has strongly rekindled this interest
\cite[see, e.g.,][and references there]{Frank:etal:2013,Lau:etal:2017,Kirov:Cotterell:2018,McCoy:etal:2018,Pater:2018}.

However, until now
systematic studies of the linguistic knowledge discovered by such
models has been limited to models that assume discrete words as
input. This is problematic in at least two
respects: \begin{inparaenum}[i)]
\item from a language
  acquisition point of view: discovering words and other linguistic
  units \emph{is} part of the linguistic knowledge the learner must
  acquire. 
\item Moreover, the choice of (orthographically delimited) words as
  the basic unit is arbitrary: a more realistic (and useful picture)
  includes multiple, non-overlapping segmentations into phonological
  constituents, morphemes, words, phrases, constructions,
  etc. (indeed, in both agglutinating and polysynthetic languages, it
  is not even clear the notion of word is useful).
\end{inparaenum}

Alternative: character-based NLMs (CNMLs). They only make assumption
that a continuous input string has been split into discrete atomic
units such as phones, and we can assume children do this very early.

We study what linguistic knowledge CNMLs induce, simulating a
minimum-previous knowledge setup by training on corpora where white
space has been removed, thus without explicit cues to word
boundaries. We find that such models are discovering a remarkable
amount of linguistic information, at the phonotactic, lexical,
morphological, syntactic and semantic levels. This suggests that,
given abundant input, a learner can induce much linguistic knowledge
with almost no innate bias towards linguistic structures and,
intriguingly, without an explicit lexicon.
