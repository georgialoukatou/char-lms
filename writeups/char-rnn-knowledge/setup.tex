\section{Experimental setup}
\label{sec:setup}

We downloaded full Wikipedia dumps for English, German, and Italian, and extracted plain text with WikiExtractor.\footnote{\url{https://github.com/attardi/wikiextractor}}
For each language, we randomly extracted testing and validation sections consisting of 50,000 paragraphs each, and used the remainder as training sets.
The training sets contained 16M (German), 9M (Italian), and 41M (English) paragraphs, corresponding to 819M (German), 463M (Italian), and 2,333M (English) words.
Order of paragraphs was shuffled for training; we did not attempt to split by sentences.
All characters were lower-cased.
For word segmentation and word-based language models, we tokenized and tagged all corpora using TreeTagger.\footnote{\url{http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/}}

For the vocabulary, we extracted the most frequent characters from
each corpus, setting thresholds so as to ensure that all characters
representing phonemes of the languages were included, resulting in vocabularies of 60 (English), 73 (German), and 59 (Italian) characters.
We further constructed \emph{word-level neural language models} (WNLMs); 
their vocabulary included the most frequent 50,000 words per corpus.

We constructed CNLMs using vanilla RNN cells and using LSTM cells; we will refer to these as RNN and LSTM, respectively.
We used LSTM cells for WNLMs.
For each model and each language, we specified a range of hyperparameter settings, and applied random search to find models that performed best on the validation set.
We terminated training after 72 hours; none of the models had overfitted, as measured by performance on the validation set.

%<<<<<<< HEAD
Language modeling performance on the test partitions is shown in
Table~\ref{tab:lm-results}.  Direct comparison with the state of the art is hindered by
the fact that we train on text without whitespace: Whitespace is both
relatively easy to predict, and it makes predicting other
characters easier. Consequently, the fact that our character-level models are
below the state of the art is expected. For example, the best model of
\newcite{merity2018analysis} achieved 1.232 BPC on enwiki8
\cite{hutter2018}, a dataset also derived from English
Wikipedia. % (Hutter 2018). %, and 1.175 on a version of PTBenglish 0.85 german 0.9, italian 0.82
On EuroParl data, \newcite{cotterell2018all} report 0.85 for English,
0.9 for German, and 0.82 for Italian. Still, our English BPC is
comparable, for example, to that reported by \newcite{Graves:2014} for
his static character-level LSTM trained on space-delimited Wikipedia data,
suggesting that we are working with reasonably high-performance language
models.\footnote{Training our models on text with whitespace, without further hyperparameter tuning to adjust to that setting, resulted in BPCs of 1.32 BPC (English), 1.28 (German), and 1.24 BPC (Italian).}
%\footnote{Training our models on text with whitespace, without further hyperparameter tuning to adjust to that setting, resulted in cross-entropies of 0.91, 1.32 BPC (English), 0.89, 1.28 BPC (German), and 0.86, 1.24 BPC (Italian).}
The perplexity of the word-level model might not be comparable to
that of highly-optimized state-of-the-art architectures, but it is at the
expected level for a well-tuned vanilla LSTM language model. For
example, \newcite{Gulordava:etal:2018} report 51.9 and 44.9 perplexities respectively in English and Italian for
their best LSTMs trained on Wikipedia data with the same vocabulary
size as ours.
%=======
%Performance on the test partitions is shown in Table~\ref{tab:lm-results}.
%Direct comparison with the state-of-the-art in character-based language modeling is hindered by the fact that we train on text without whitespace.
%The best models of \cite{merity2018analysis} achieved 1.232 BPC on enwiki8 \cite{hutter2018}, a dataset also derived from English Wikipedia. % (Hutter 2018). %, and 1.175 on a version of PTBenglish 0.85 german 0.9, italian 0.82
%On Europarl data, \cite{cotterell2018all} report 0.85 for English, 0.9 for German, and 0.82 for Italian. 
%Our BPC values are higher, but this is expected given that we do not provide whitespace to the model: Whitespace is both relatively easy to predict, and it makes predicting other characters easier.\footnote{Refitting our models to data with whitespace, without retuning hyperparameters, yields ....}
%>>>>>>> 86b9fd533dc18bab83a010158126d7366aae3681

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l|l}
      \multicolumn{1}{c}{}&\emph{LSTM}&\emph{RNN}&\emph{Word LSTM}\\
      \hline
	    English & 1.62 & 2.08 & 48.99  \\
	    German &  1.51 & 1.83 & 37.96   \\
	    Italian & 1.47 & 1.97 & 42.02  \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:lm-results} Performance of language models. For CNLMs, we report bits-per-character (BPC). For word-based models, we report perplexity.}
\end{table}

%\begin{table}[t]
%  \begin{center}
%    \begin{tabular}{l|l|l|l|l}
%      \multicolumn{1}{c}{}&\emph{LSTM}&\emph{RNN}&\emph{Word LSTM}\\
%      \hline
%	    English & 1.12 / 1.62 & 1.44 / 2.08 & 3.89 / 48.99  \\
%	    German &  1.05 / 1.51 & 1.27 / 1.83 & 3.63 / 37.96   \\
%	    Italian & 1.02 / 1.47 & 1.37 / 1.97 & 3.85 / 42.02  \\
%    \end{tabular}
%  \end{center}
%  \caption{\label{tab:lm-results} Performance of language models. For CNLMs, we report cross-entropy and bits-per-character (BPC). For word-based models, we report cross-entropy and perplexity.}
%\end{table}





