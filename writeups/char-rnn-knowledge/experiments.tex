\section{Experiments}
\label{sec:experiments}

\subsection{Discovering phonotactic constraints}
\label{sec:phonotactics}

Focusing on German and Italian, that have reasonably transparent
orthographies.

How setup here differs from general one.

Method: construct pairs of letter bigrams (corresponding to phoneme
bigrams) beginning with the same letter, such that one is
phonotactically acceptable in the language and the other isn't, but
the independent unigram probability of the unacceptable bigram is
higher than that of the acceptable one. E.g., ``\emph{br}'' is
acceptable Italian sequence, ``\emph{bt}'' isn't, although
\emph{``t''} is more frequent. We re-train the CNLM on a version of
the corpus from which both bigrams have been removed. We then look at
the likelihood the model assigns to both sequences. If the model
assigns a larger probability to the correct sequence, it means that it
implicitly possesses a notion of phonological categories such as
stops and sonorants, which allows it to correctly generalize from
attested (e.g., ``\emph{tr}'') sequences to unattested ones
(\emph{``br''}).

LSTM vs RNN

Results, in a table with all pairs for both languages, as in Table
\ref{tab:phonotactics-results}.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{ll|cc}
      \multicolumn{2}{c}{\emph{bigrams}}&\emph{LSTM}&\emph{RNN}\\
      \hline
      \multicolumn{4}{c}{\emph{German}}\\
      \hline
      \ldots & \ldots & \ldots & \ldots \\
      \ldots & \ldots & \ldots & \ldots \\
      \hline
      \multicolumn{4}{c}{\emph{Italian}}\\
      \hline
      pa & pb & \ldots & \ldots \\
      \ldots & \ldots & \ldots & \ldots \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:phonotactics-results} Figure of merit could be
    log likelihood ratio between acceptable and unacceptable bigram,
    with positive values in bold.}
\end{table}


Discussion: note that generalization of model are purely
distributional, with no aid from perceptual or articulatory cues.


\subsection{Word segmentation}
\label{sec:segmentation}

English/German/Italian

Specifics of this setup, and differences from general setup

Does the model develop an implicit notion of word?

Should we use only one method here? One that is unsupervised, for
direct comparison to the Bayesian approach? We could then report that,
when training on the hidden state, we get super-high accuracy,
indicating that the information is there.

LSTM vs RNN

Results could be summarized as in Table \ref{tab:segmentation-results}.


\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|lll|lll|lll}
      \multicolumn{1}{c}{}&\multicolumn{3}{c}{\emph{LSTM}}&\multicolumn{3}{c}{\emph{RNN}}&\multicolumn{3}{c}{\emph{Bayes}}\\
      \hline
      &P&R&F&P&R&F&P&R&F\\
      \hline
      English &\ldots & \ldots & \ldots &\ldots & \ldots & \ldots&\ldots & \ldots & \ldots\\
      German &\ldots & \ldots & \ldots &\ldots & \ldots & \ldots&\ldots & \ldots & \ldots\\
      Italian &\ldots & \ldots & \ldots &\ldots & \ldots & \ldots&\ldots & \ldots & \ldots\\
    \end{tabular}
  \end{center}
  \caption{\label{tab:segmentation-results} Obviously, we must reduce space between P, R and F}
\end{table}

Report syntactic depth plot: illustrates how it is useful for
segmentation knowledge to be implicit, as the model ``knows'' about
different kinds of boundaries in a continuous manner.

Qualitative analysis: report proportions of over- and
under-segmentation for our best CNLM, look at common over- and
under-segmentation errors in English? E.g., I could go manually
through the top 30 ones, say...

\subsection{Discovering morphological categories}
\label{sec:categories}

Focusing on German and Italian given massive morphosyntactic ambiguity
and impoverished morphology of English. Note that these are lexical
properties, probed in a model that has no explicit notion of word!

\paragraph{Word classes (nouns vs.~verbs)}

(Departures from general setup)

Procedure as described in the quip.

Baselines: autoencoder, word-based NLM embeddings, also LSTM vs
RNN. Outperforming autoencoder shows that model has learned categories
based on broader distributional evidence, not just typical strings
cueing nouns and verbs.

General accuracy results table for fixed N of training examples as in Table \ref{tab:pos-results}.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l}
      \multicolumn{1}{c}{}&\emph{German}&\emph{Italian}\\
      \hline
      LSTM&\ldots&\ldots\\
      RNN&\ldots&\ldots\\
      Autoencoder&\ldots&\ldots\\
      WordNLM&\ldots&\ldots\\
    \end{tabular}
  \end{center}
  \caption{\label{tab:pos-results} \ldots}
\end{table}

Report CNLM vs baseline comparison in function of training examples as in the Quip (possibly for a language only).

\paragraph{Number}

Does the hidden state of the CNLM store an abstract notion of
number. German nouns can be binned in different classes depending on
the morpheme or morphological process they use to form the plural. We
train a number classifier on a subset of these classes, test on the
others: if model generalizes correctly, it means that it knows about
number independently of its surface expression.

Specifics of data-set construction and classifier training. Pick only one setup, e.g., training on -n, -s, and -e.

Baselines/comparisons: LSTM vs RNN, autoencoder, word-based NLM.

Results could be summarized as in Table \ref{tab:number-results}.


\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l}
      \multicolumn{1}{c}{}&\emph{training plurals}&\emph{-r}&\emph{no-suffix}\\
      \hline
      LSTM&\ldots&\ldots\\
      RNN&\ldots&\ldots&\ldots\\
      Autoencoder&\ldots&\ldots&\ldots\\
      WordNLM&\ldots&\ldots&\ldots\\
    \end{tabular}
  \end{center}
  \caption{\label{tab:number-results} Figure of merit is accuracy for plural prediction.}
\end{table}


Control follow-up: singular nouns with plural ending (with similar table?).

\subsection{Capturing syntactic dependencies}
\label{sec:dependencies}

Despite not having pre-defined information about words and morphemes,
is the model able to capture non-adjacent syntactic dependencies? NB:
actually for a CNLM even \emph{``\textbf{la} bell\textbf{a}''} is long
distance! Constructions will be language-specific, so we discuss
German and Italian separately (not much in English).

As usual, specifics of training etc that depart from general setup.

\paragraph{German} We consider 4 constructions:
\begin{inparaenum}[i)]
\item article-noun gender agreement, possibly with material in the middle,
\item determiner-noun case concord, again with material in the middle,
\item preposition case sub-categorization, with material in the middle.
\end{inparaenum}

Discussion case-by-case, including how do we control for possible
n-gram effects and length. \textbf{It's not clear to me how the n-gram
  control model would work in each of these cases, can you explain in
  detail?}

How to present results: figures, with number of intervening words
(from 0 to 2 or 3) on x axis, accuracy on y axis. Multiple figures
when different genders or cases are tested. Within the figures, one
line per model: LSTM, RNN, n-gram, wordNLM.

\paragraph{Italian} We consider further constructions from Italian,
that confirm the results we got in German.
\begin{inparaenum}[i)]
\item article-noun gender agreement with material in the middle,
\item article-adjective gender agreement, with an adverb in the middle,
\item article-adjective  agreement, with an adverb in the middle.
\end{inparaenum}

Discussion case-by-case, including how we control for n-gram frequency
and length (NB: if we want to do article-noun gender agreement with
nothing in the middle, we must insert n-gram control).

Results: same figure format as above? Or table with a row for each
pattern and a column for each model?


\subsection{Lexical semantic similarity}
\label{sec:similarity}

In English, because that's where we have resources available.

Correlation with one or more word similarity sets.

Comparison to word-based NLM (rather than word2vec or such, which is
specifically tuned for semantics).

