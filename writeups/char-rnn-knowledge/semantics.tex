

\subsection{Semantics}
\label{sec:semantics}

Finally, we probe the CNLMs knowledge of semantics. We turn to English
because more resources are available there, and more specifically we
decided to focus on the Microsoft Research Sentence Completion task
\cite{Zweig:Burges:2011}. The challenge consists of sentences with a
gap, and a 5-word multiple choice to fill the gap. Picking the right
word requires a mixture of syntax, lexical semantics, world knowledge
and pragmatics. For example, in \emph{``Was she his [
  \underline{client}|musings|discomfiture|choice|opportunity], his
  friend , or his mistress?}, the model should realize that the
missing word is coordinated with \emph{friend} and \emph{mistress},
and that the latter are human beings. We chose this challenge because
language models can be easily applied by calculating the likelihood of
all possible completions and selecting the one with the highest
likelihood.

The domain of the task (Sherlock Holmes novels) is very different from
the Wikipedia data-set we are using (e.g., due to prevalence of dialogue); thus we additionally trained our
models on the training set provided for the task, consisting of 19th
century English novels. 
%We both consider a fresh model trained on
%that data, and initializing it with the Wikipedia model.
\footnote{For the WordNLM, the vocabulary consisted
of the 50,000 most common words in the in-domain training set.}
%For comparison, we report results (KN5 from , LSTM from ) from previous work that were trained on the 19th century novels dataset (but the LSTM from that work had Glove embeddings). % \cite{zhang2016top} has a nice table if we want to report more

Results are shown in Figure~\ref{tab:msr-completion-results}.  The
models trained on Wikipedia perform poorly but above chance,
reflecting the domain mismatch.  When trained on data from the
appropriate domain, the LSTM CNLM outperforms many previously reported
results from word-level neural
models. %, and approaches the best published results.
%, held by approaches developed for the completion task \cite{woods2016exploiting}.
% The best results I could find, https://github.com/ctr4si/sentence-completion, are much better than the best peer-reviewed published ones
The vanilla RNN is not successful even when trained on the in-domain
data, contrasting with \emph{word}-based vanilla RNNs, whose
performance, while below that of LSTMs, is much stronger.
The WordNLM shows a slight boost over the CNLM.\footnote{Results were significantly degraded (50.1 \%) when using the Wikipedia vocabulary instead.}

This experiment shows that a CNLM, trained without word boundaries, learns forms of semantic/world knowledge to a degree competitive with models trained on words.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l}
   %   \multicolumn{1}{c}{}& Model \\
        LSTM 	    &      34.1/59.0 \\ % /59.2
	    RNN  &     24.3/24.0 \\ % /27.1
	    WordNLM & 37.1/63.3 \\ \hline \hline % 50.1/52.4/
	    Random & 20 \\ \hline
	    KN5   & 40.0 \\
            Word RNN & 45.0 \\
	    Word LSTM & 56.0 \\ 
	    LdTreeLSTM  & 60.67 \\	    \hline
Skipgram + RNNs  & 58.9 \\
            \citet{woods2016exploiting} &  61.4 \\
\citet{melamud2016context2vec} & 65.1 \\
    \end{tabular}
  \end{center}
	\caption{\label{tab:msr-completion-results} Results on MSR Sentence Completion. For our models, we show numbers for the Wikipedia model and for a model trained on in-domain data. We compare with language models from prior work: Kneser-Ney 5-gram model \cite{Mikolov:2012}, Word RNN \cite{zweig2012computational}, Word LSTM and LdTreeLSTM \cite{zhang2016top}. We further report models incorporating distributional encodings of semantics: Skipgram+RNNs from \newcite{Mikolov:etal:2013b}, the PMI-based model of \citet{woods2016exploiting}, and a context-embedding based approach by \citet{melamud2016context2vec}.}
\end{table}

% , (3) the Wikipedia model posttrained on the in-domain data
% For the WordNLM, we additionally provide accuracy for a model with vocabulary derived from the in-domain training data. 
% \textbf{Explain what these are}


