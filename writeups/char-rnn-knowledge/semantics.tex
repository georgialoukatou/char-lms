

\subsection{Lexical semantics}
\label{sec:semantics}

Finally, we probe the CNLMs knowledge of lexical semantics.
We turn to  English because more resources are available there.

We use the Microsoft Research Sentence Completion task \cite{Zweig:Burges:2011}.
The challenge consists of sentences with a gap, and five words; the model has to choose which word is most appropriate.
The choice is mostly between words of the same syntactic category, and solving the task requires world knowledge for humans to solve; thus this complements the morphosyntactic tests in the previous section.
Language models can be applied to this task by calculating the likelihood of all possible completions of a sentence and selecting the one assigned the highest likelihood.

The domain of the task (Sherlock Holmes novels) is very different from the Wikipedia data-set we are using; thus we additionally trained our models on the training set provided for the task, consisting of 19th century English novels.
We both consider a fresh model trained on that data, and initializing it with the Wikipedia model.
%For comparison, we report results (KN5 from , LSTM from ) from previous work that were trained on the 19th century novels dataset (but the LSTM from that work had Glove embeddings). % \cite{zhang2016top} has a nice table if we want to report more

Results are shown in Figure~\ref{tab:msr-completion-results}.
The models trained on Wikipedia perform poorly but above chance, reflecting the domain mismatch.
When trained on data from the appropriate domain, the LSTM CNLM outperforms many previously reported results from word-level neural models. %, and approaches the best published results.
%, held by approaches developed for the completion task \cite{woods2016exploiting}.
% The best results I could find, https://github.com/ctr4si/sentence-completion, are much better than the best peer-reviewed published ones
The vanilla RNN is not successful even when trained on the in-domain data, contrasting with \emph{word}-based vanilla RNNs, whose performance, while below that of LSTMs, is much stronger.

This experiment shows that a CNLM, trained without word boundaries, learns lexical knowledge to a degree competitive with models trained on words.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l|llllll}
      \multicolumn{1}{c}{}& Model \\
LSTM CNLM	    &      34.1/59.0/59.2 \\
	    RNN CNLM &     24.3/24.0/27.1 \\
	    WordNLM & 37.1/.../... \\ \hline
	    Random & 20 \\
	    KN5   & 40.0 \\
            Word RNN & 45.0 \\
	    WordNLM  & 55.96 \\
Skipgram + RNNs  & 58.9 \\
LdTreeLSTM  & 60.67 \\
            \citet{woods2016exploiting} &  61.44 \\
\citet{melamud2016context2vec} & 65.1 \\
    \end{tabular}
  \end{center}
  \caption{\label{tab:msr-completion-results} Results on MSR Sentence Completion. KN5 is from \cite{Mikolov:2012}, Word LSTM and LdTreeLSTM are from\cite{zhang2016top}, Skipgram+RNNs from \cite{Mikolov:etal:2013b}.}
\end{table}


