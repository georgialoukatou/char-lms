

\subsection{Semantics}
\label{sec:semantics}

Finally, we probe the CNLMs knowledge of semantics. We turn to English
because more resources are available there, and more specifically we
decided to focus on the Microsoft Research Sentence Completion task
\cite{Zweig:Burges:2011}. The challenge consists of sentences with a
gap, and a 5-word multiple choice to fill the gap. Picking the right
word requires a mixture of syntax, lexical semantics, world knowledge
and pragmatics. For example, in \emph{``Was she his [
  \underline{client}|musings|discomfiture|choice|opportunity], his
  friend , or his mistress?}, the model should realize that the
missing word is coordinated with \emph{friend} and \emph{mistress},
and that the latter are human beings. We chose this challenge because
language models can be easily applied by calculating the likelihood of
all possible completions and selecting the one with the highest
likelihood.

The domain of the task (Sherlock Holmes novels) is very different from
the Wikipedia data-set we are using; thus we additionally trained our
models on the training set provided for the task, consisting of 19th
century English novels.  We both consider a fresh model trained on
that data, and initializing it with the Wikipedia model.
For the WNLM, we additionally created a fresh model whose vocabulary consisted
of the 50,000 most common words in the in-domain training set.
%For comparison, we report results (KN5 from , LSTM from ) from previous work that were trained on the 19th century novels dataset (but the LSTM from that work had Glove embeddings). % \cite{zhang2016top} has a nice table if we want to report more

Results are shown in Figure~\ref{tab:msr-completion-results}.  The
models trained on Wikipedia perform poorly but above chance,
reflecting the domain mismatch.  When trained on data from the
appropriate domain, the LSTM CNLM outperforms many previously reported
results from word-level neural
models. %, and approaches the best published results.
%, held by approaches developed for the completion task \cite{woods2016exploiting}.
% The best results I could find, https://github.com/ctr4si/sentence-completion, are much better than the best peer-reviewed published ones
The vanilla RNN is not successful even when trained on the in-domain
data, contrasting with \emph{word}-based vanilla RNNs, whose
performance, while below that of LSTMs, is much stronger.
The WNLM suffers from the domain mismatch in the vocabulary; using the in-domain vocabulary gives a slight boost over the CNLM.

This experiment shows that a CNLM, trained without word boundaries, learns forms of semantic/world knowledge to a degree competitive with models trained on words.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{l|l|l|l|llllll}
      \multicolumn{1}{c}{}& Model \\
LSTM CNLM	    &      34.1/59.0/59.2 \\
	    RNN CNLM &     24.3/24.0/27.1 \\
	    WordNLM & 37.1/50.1/52.4/63.3 \\ \hline
	    Random & 20 \\
	    KN5   & 40.0 \\
            Word RNN & 45.0 \\
	    WordNLM  & 55.96 \\
Skipgram + RNNs  & 58.9 \\
LdTreeLSTM  & 60.67 \\
            \citet{woods2016exploiting} &  61.44 \\
\citet{melamud2016context2vec} & 65.1 \\
    \end{tabular}
  \end{center}
	\caption{\label{tab:msr-completion-results} Results on MSR Sentence Completion. For our models, we show numbers for (1) the Wikipedia model, (2) a fresh model trained on in-domain data, (3) the Wikipedia model posttrained on the in-domain data. For the WNLM, we additionally provide accuracy for a model with vocabulary derived from the in-domain training data. We compare with language models from prior work: Kneser-Ney 5-gram model, \newcite{Mikolov:2012}, Word LSTM and LdTreeLSTM from \newcite{zhang2016top}, Skipgram+RNNs from \newcite{Mikolov:etal:2013b}. We further report models specifically designed to capture semantics: a PMI-based model from \citet{woods2016exploiting}, and a context-embedding based approach by \citet{melamud2016context2vec}.}
\end{table}

% \textbf{Explain what these are}


